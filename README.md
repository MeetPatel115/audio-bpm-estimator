# ğŸµ Beat Per Minute Prediction

Predict **Beats Per Minute (BPM)** from audio-related features using classic ML models.  
This repo includes **EDA â†’ Feature Engineering â†’ Model Training â†’ Test Prediction** in a single Jupyter notebook.

## ğŸ“‚ Project Structure
```
.
â”œâ”€â”€ Day2_annotated.ipynb   # Notebook with comments for every step
â”œâ”€â”€ train.csv              # Training dataset (features + target BeatsPerMinute)
â”œâ”€â”€ test.csv               # Test dataset (features only)
â”œâ”€â”€ sample_submission.csv  # Expected submission format
â”œâ”€â”€ submission1.csv        # Predictions generated by the notebook
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # Project documentation
```

## ğŸ§  Problem
Given tabular features describing tracks, predict the **`BeatsPerMinute`** for each record in the test set.

## ğŸ› ï¸ Setup
```bash
git clone <your-repo-url>
cd <your-repo-folder>
python -m venv .venv
# Windows
.venv\Scripts\activate
# macOS/Linux
source .venv/bin/activate

pip install -r requirements.txt
```

## ğŸš€ Run
Open the notebook and execute cells top-to-bottom:
```bash
jupyter notebook Day2_annotated.ipynb
```
This will:
1. Load `train.csv` and `test.csv`
2. Perform EDA
3. Apply feature engineering (`create_features`)
4. Split into train/validation (80/20)
5. Train a baseline model (e.g., Linear Regression, Random Forest)
6. Evaluate with **RMSE** and **RÂ²**
7. Generate `submission1.csv` with columns: `id, BeatsPerMinute`

## ğŸ“Š Notes on Modeling
- Baseline uses **Linear Regression** (change to **RandomForestRegressor** / **GradientBoostingRegressor** to compare)
- Metric: **RMSE** (lower is better), **RÂ²** (closer to 1 is better)
- You can add cross-validation and hyperparameter tuning for stronger results

## ğŸ§© Feature Engineering (examples)
- `Rhythm_Audio_Interaction = RhythmScore * AudioLoudness`
- `Vocal_Acoustic_Ratio = VocalContent / AcousticQuality` (handle division by zero)
- Median imputation for missing numeric values

## ğŸ”® Next Improvements
- Hyperparameter tuning (Random Forest / XGBoost / LightGBM)
- Feature importance analysis; drop noisy features
- K-fold cross-validation for robustness
- Try regularized linear models (Ridge/Lasso) and SVR

## ğŸ“¦ Dependencies
See `requirements.txt` (generated from notebook imports), including:
- pandas, numpy, matplotlib, seaborn
- scikit-learn, scipy
- ipykernel for Jupyter

## ğŸ‘¤ Author
**Meet Patel** â€” Toronto, ON

---
If you find this useful, â­ the repo and open an issue/PR with suggestions!
